{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version 1.3.0\n",
      "torch.cuda.is_available() False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carsonlam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math, time, os, datetime, shutil, pickle, sys\n",
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scripts.MoveData import *\n",
    "from scripts.Transformer import *\n",
    "from scripts.TalkTrain import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet') \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('torch.version',torch.__version__)\n",
    "print('torch.cuda.is_available()',torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is working version of the TransformerNTM, I show that with long tapered training starting from 0.01 and tapering to very small learning rates ~ 0.0005, you can get a Transformer chatbot to memorize your name and you can change your name and the chatbot will remember your new name. The final loss was loss = 0.010 for this to happen after 400 - 800 epochs, which is waay too many. \n",
    "\n",
    "The [Neural Turing Machine paper](https://arxiv.org/pdf/1410.5401.pdf) explains the components of neural memory. [This blog](https://rylanschaeffer.github.io/content/research/neural_turing_machine/main.html) does a great job expalaining the paper to technical non-researchers. \n",
    "\n",
    "## Addressing\n",
    "Adressing is creating weight vectors across the rows of the memory to determine where to read and write. Each stage generates an intermediate weight vector that gets passed to the next stage. First is content addressing:\n",
    "\n",
    "### Content Adressing \n",
    "generates a weight vector based on how similar each row in memory is to a length-C vector key k_t emitted by the controller\n",
    "\n",
    "<img src=\"https://rylanschaeffer.github.io/content/research/neural_turing_machine/ntm_addr_1.png\">\n",
    "\n",
    "For each head, the controller produces a key vector kt that is compared to each row of Mt using a similarity measure. In this paper, the authors use cosine similarity\n",
    "\n",
    "$$ K(k_t, M_t(i)) = \\frac{k_t \\cdot M_t(i)}{\\|k_t\\| \\cdot \\|M_t(i)\\|}$$ \n",
    "\n",
    "The PyTorch version of this formula is \n",
    "\n",
    "`F.cosine_similarity(self.memory + 1e-16, k + 1e-16, dim=-1)`\n",
    "\n",
    "The variable `wc` in ` wc = self._similarity(k, beta)` is the weighted softmax of these similarities and can be used as and an attention weighting over the rows of the matrix based on similarity to a generated vector k. Larger betas cause the distribution over the rows of the memory to be more concentrated on the highest cosine similarity row, thus beta is called the key strength or focus.\n",
    "\n",
    "$$w_t^c(i) = \\frac{exp\\Big(\\beta_t K (k_t, M_t(i))\\Big)}{\\sum_j exp\\Big(\\beta_t K(k_t, M_t(j))\\Big)}$$\n",
    "\n",
    "`wc = F.softmax(beta * F.cosine_similarity(self.memory + 1e-16, k + 1e-16, dim=-1), dim=1)`\n",
    "\n",
    "## location-based addressing\n",
    "\n",
    "In some cases, we may want to read from specific memory locations instead of looking for specific memory values. The example the authors give is the function f(x,y)=x∗y. In this case, we don't care what the values of x and y are, just that x and y are consistently read from the same memory locations. This is called location-based addressing, and to implement it, we'll need three more stages. In the second stage, a scalar parameter g ∈ (0,1), called the interpolation gate, blends the content weight vector wc with the previous time step's weight vector w_t−1 to produce the gated weighting wg. This allows the system learn when to use (or ignore) content-based addressing.\n",
    "\n",
    "<img src=\"https://rylanschaeffer.github.io/content/research/neural_turing_machine/ntm_addr_2.png\">\n",
    "\n",
    "$$w_t^g \\leftarrow g_t w_t^c + (1- g_t) w_{t-1}$$\n",
    "\n",
    "                                              wg = g * wc + (1 - g) * w_prev\n",
    "                                              \n",
    "## Shift\n",
    "\n",
    "s - Shift weighting (batch_size, memory_n) (sums to 1)\n",
    "\n",
    "We'd like the controller to be able to shift focus to other rows. Let's suppose that as one of the system's parameters, the range of allowable shifts is specified. For example, a head's attention could shift forward a row (+1), stay still (0), or shift backward a row(-1). \n",
    "\n",
    "<img src=\"https://rylanschaeffer.github.io/content/research/neural_turing_machine/ntm_addr_3.png\">\n",
    "\n",
    "We'll perform the shifts modulo R so that a shift forward at the bottom row of memory moves the head's attention to the top row, and similarly for a shift backward at the top row. After interpolation, each head emits a normalized shift weighting st, and the following convolutional shift is performed to produce the shifted weight w_hat\n",
    "\n",
    "$$\\tilde{w}_t(i) \\leftarrow \\sum\\limits_{j=0}^{R-1} w_t^g(j) s_t(i-j)$$\n",
    "\n",
    "                           F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n",
    "\n",
    "I never liked this notation for convolution. it leaves alot unsaid. think of s as being sliding window dot product, if you have already done the intro to pytorch lesson with 2D image convolutions, you might think of s as a filter of size 3 being applied to the image wg. To create the wrap-around padding effect, the last element is appended to the beginning and the first element is appended to the end. since the filter size is 3, this is just the right padding to result in an output tensor of the same size as the input. \n",
    "\n",
    "The F.conv1D function takes an input of shape (batch_size, input channels, sequence length), filter of shape (output channels, input channels, filter length) and outputs a tensor of shape (batch_size, output channels, output sequence length) \n",
    "\n",
    "If you are wondering why we have a for loop that goes through each sample in the batch, it is because we are not sharing weights across samples, each sample's filter is indenpendant and is part of its own independant history of states and actions, this is not a 1:1 mapping task where we are doing the same task for every sample in the batch \n",
    "\n",
    "The fourth and final stage, sharpening, is used to prevent the shifted weight w_hat from blurring. To do this, a scalar gamma >= 1 is required\n",
    "\n",
    "$$w_t(i) \\leftarrow \\frac{\\tilde{w}_t(i)^{\\gamma_t}}{\\sum\\limits_j \\tilde{w}_t(j)^{\\gamma_t}}$$\n",
    "\n",
    "## Writing \n",
    "\n",
    "$$\\mathcal{M}_t^{erased}(i) \\leftarrow \\mathcal{M}_{t-1}(i)[\\mathbf{1} - w_t(i) e_t ]$$\n",
    "\n",
    "$$\\mathcal{M}_t(i) \\leftarrow \\mathcal{M}_t^{erased}(i) + w_t(i) a_t$$\n",
    "\n",
    "$$\\mathcal{M}_t(i) \\leftarrow \\mathcal{M}_{t-1}(i)[\\mathbf{1} - w_t(i) e_t ] + w_t(i) a_t $$\n",
    "\n",
    "The initial write function sequentially overwrites rows in the memory whereas the another possibility \n",
    "might be to intelligently choose which row to overwrite by learning a write weighting ww and erase vector e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    \"\"\" Neural Turing Machine Memory\"\"\"\n",
    "    def __init__(self, N, M, controller_size):\n",
    "        \n",
    "        \"\"\"Initialize the Memory matrix.\n",
    "        The memory's dimensions are (batch_size x N x M).\n",
    "        Each batch has it's own memory matrix.\n",
    "        N: Number of rows in the memory.\n",
    "        M: Number of columns/features in the memory.\n",
    "        \"\"\"\n",
    "        super(NTM, self).__init__()\n",
    "\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.controller_size = controller_size\n",
    "\n",
    "        self.memory0 = torch.ones(self.N,self.M).abs_()*1e-6\n",
    "\n",
    "        # create Fully Connected layer for addressing using controller output\n",
    "        self.address_param_sizes = [self.M, 1, 1, 3, 1]\n",
    "        self.addresses =  nn.Linear(self.controller_size, \n",
    "                                    sum(self.address_param_sizes))  \n",
    "\n",
    "    def reset(self, batch_size):\n",
    "        \"\"\"Reset the memory\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.write_loc = 0\n",
    "        self.memory = self.memory0.clone().repeat(batch_size, 1, 1)\n",
    "        \n",
    "    def write(self, a):\n",
    "        \n",
    "        ww = torch.zeros(a.size(0), self.N)\n",
    "        ww[:, self.write_loc] = 1.0\n",
    "        e = torch.ones(a.size(0), self.M)\n",
    "\n",
    "        erase = torch.matmul(ww.unsqueeze(-1), e.unsqueeze(1))\n",
    "        add = torch.matmul(ww.unsqueeze(-1), a.unsqueeze(1))\n",
    "\n",
    "        # write to memory\n",
    "        self.memory = self.memory * (1 - erase) + add\n",
    "        self.write_loc = (self.write_loc + 1) % self.N\n",
    "\n",
    "    def read(self, wr):\n",
    "        \"\"\"Read from memory (according to section 3.1)\"\"\"\n",
    "        return torch.matmul(wr.unsqueeze(1), self.memory).squeeze(1)\n",
    "        \n",
    "    def address(self, controller_output, w_prev):\n",
    "        \"\"\"NTM Addressing (according to section 3.3)\n",
    "           both w_prev and ware Softmax weightings over rows \n",
    "           of the memory matrix with shapes (batch_size, memory_n)\n",
    "        input:\n",
    "            controller_output- (batch_size, controller_size)\n",
    "            w_prev - The weighting produced in the previous time step\n",
    "        output:\n",
    "            w - new Softmax weighting over rows of the memory matrix\n",
    "        \"\"\"\n",
    "        address_params = self.addresses(controller_output)\n",
    "\n",
    "        k, beta, g, s, gamma = self.split_cols(address_params, \n",
    "                                               self.address_param_sizes)\n",
    "        \"\"\"\n",
    "        k - The key vector (batch_size, memory_m) (a vector)\n",
    "        beta - The key strength (focus) (batch_size, 1) (0,infinity)\n",
    "        g - Scalar interpolation gate with w_prev (batch_size, 1) (0,1)\n",
    "        s - Shift weighting (batch_size, memory_n) (sums to 1)\n",
    "        gamma - Sharpen weighting scalar (batch_size, 1) (1,infinity)\n",
    "        \"\"\"\n",
    "        beta = F.softplus(beta)\n",
    "        g = torch.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        gamma = 1 + F.softplus(gamma)\n",
    "        # Content Addressing\n",
    "        wc = self._similarity(k, beta)\n",
    "        # Location Adressing\n",
    "        wg = self._interpolate(w_prev, wc, g)\n",
    "        w_hat = self._shift(wg, s)\n",
    "        wr = self._sharpen(w_hat, gamma)\n",
    "  \n",
    "        return wr\n",
    "\n",
    "    def split_cols(self, mat, lengths):\n",
    "        \"\"\"Split a 2D matrix to variable length columns.\"\"\"\n",
    "        assert mat.size()[1] == sum(lengths), \"Lengths must be summed to num columns\"\n",
    "        l = np.cumsum([0] + lengths) # [ 0, 20, 21, 22, 25, 26]\n",
    "        results = []\n",
    "        for s, e in zip(l[:-1], l[1:]):  # 0 20, 20 21, ... \n",
    "            results += [mat[:, s:e]]\n",
    "        return results\n",
    "    \n",
    "    def _similarity(self, k, beta):\n",
    "        k = k.view(self.batch_size, 1, -1)\n",
    "        w = F.softmax(beta * F.cosine_similarity(self.memory+1e-16, \n",
    "                                                 k+1e-16,dim=-1),dim=1)\n",
    "        return w\n",
    "\n",
    "    def _interpolate(self, w_prev, wc, g):\n",
    "        return g * wc + (1 - g) * w_prev\n",
    "\n",
    "    def convolve(self, w, s):\n",
    "        \"\"\"Circular convolution implementation.\"\"\"\n",
    "        assert s.size(0) == 3\n",
    "        t = torch.cat([w[-1:], w, w[:1]])\n",
    "        c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1) \n",
    "        # .view(-1) gets rid of the first two 1 dims inc\n",
    "        return c\n",
    "    \n",
    "    def _shift(self, wg, s):\n",
    "        result = torch.zeros(wg.size())\n",
    "        for b in range(self.batch_size):\n",
    "            result[b] = self.convolve(wg[b], s[b])\n",
    "        return result\n",
    "\n",
    "    def _sharpen(self, w_hat, gamma):\n",
    "        w = w_hat ** gamma\n",
    "        w = torch.div(w, torch.sum(w, dim=1).view(-1, 1) + 1e-16)\n",
    "        return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    A Neural Turing Machine controller based on LSTM\n",
    "    summarizes sequences into a hidden state that can\n",
    "    be used to generate write weights (addressing)\n",
    "    \n",
    "    we use the same LSTM to generate write weights\n",
    "    as we use to generate the vector that is stored\n",
    "    in memory, this is probably a design flaw \n",
    "    \"\"\"\n",
    "    def __init__(self, memory_module, hidden_size, num_layers):\n",
    "        \n",
    "        super(Controller, self).__init__()\n",
    "\n",
    "        self.memory_module = memory_module\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.summarizer = nn.LSTM(input_size=hidden_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.summarizer_h = nn.Parameter(torch.randn(n_layers,1,emb_dim))\n",
    "        self.summarizer_c = nn.Parameter(torch.randn(n_layers,1,emb_dim))\n",
    "        \n",
    "        self.recaller = nn.LSTM(input_size=hidden_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.recaller_h = nn.Parameter(torch.randn(n_layers,1,emb_dim))\n",
    "        self.recaller_c = nn.Parameter(torch.randn(n_layers,1,emb_dim))\n",
    "        \n",
    "        self.context = torch.randn(batch_size, 1, emb_dim)\n",
    "        self.mask = (torch.ones(batch_size, 1, 1) == 1.)\n",
    "        self.w = torch.zeros(batch_size, self.memory_module.N)\n",
    "        self.w[:,0] = 1.0 # set reader attention at first spot in the memory\n",
    "\n",
    "    def reset(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        h = self.summarizer_h.clone().repeat(1, batch_size, 1)\n",
    "        c = self.summarizer_c.clone().repeat(1, batch_size, 1)\n",
    "        self.summarizer_s = h, c \n",
    "        \n",
    "        h = self.recaller_h.clone().repeat(1, batch_size, 1)\n",
    "        c = self.recaller_c.clone().repeat(1, batch_size, 1)\n",
    "        self.recaller_s = h, c \n",
    "        \n",
    "        self.memory_module.reset(batch_size)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        out, state = self.lstm(x, prev_state)\n",
    "        return out, state\n",
    "    \n",
    "    def write2memory(self, encoding):\n",
    "        \"\"\"writes encoding to memory\"\"\"\n",
    "        summary, self.summarizer_s  = self.summarizer(encoding, self.summarizer_s)\n",
    "        last_h = summary[:,-1,:] # last layer output[:,-1,:] = end_token_hidden[-1,:,:]\n",
    "        self.memory_module.write(last_h)\n",
    "\n",
    "    def memory2context(self, encoding):\n",
    "        \"\"\"\n",
    "        uses encoding to query the memory and update the context\n",
    "        \"\"\"\n",
    "        query, self.recaller_s  = self.recaller(encoding, self.recaller_s)\n",
    "        last_h = query[:,-1,:] # last layer output[:,-1,:] = end_token_hidden[-1,:,:]\n",
    "        self.w = self.memory_module.address(last_h, self.w)\n",
    "        context = self.memory_module.read(self.w)\n",
    "        self.context = context.unsqueeze(1) # add the seq_len =1 time dimension\n",
    "\n",
    "    def detach_memory(self):\n",
    "        self.memory_module.memory = self.memory_module.memory.detach()\n",
    "        self.w = self.w.detach()\n",
    "        self.context = self.context.detach()\n",
    "        self.mask = self.mask.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, \n",
    "                 heads, mem_slots, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        dim_k = emb_dim // heads\n",
    "        self.mem_slots = mem_slots\n",
    "        \n",
    "        self.memcoder = Decoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "        \n",
    "        self.ntm = NTM(mem_slots, emb_dim, emb_dim)\n",
    "        self.controller = Controller(self.ntm, emb_dim, n_layers)\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        self.controller.detach_memory()\n",
    "        self.incoding = self.memcoder(in_toks, in_mask, self.controller.context, self.controller.mask)\n",
    "        self.dout = self.decoder(out_toks, out_mask, self.incoding, in_mask)\n",
    "        output = self.out(self.dout)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options(batchsize=1, device = torch.device(\"cpu\"), epochs=50, lr=0.01, \n",
    "              max_len = 25, save_path = 'weights/ntm_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../../saved/memory.json', opt=opt)\n",
    "\n",
    "emb_dim, n_layers, heads, mem_slots, dropout, batch_size = 8, 1, 3, 4, 0.001, 1\n",
    "\n",
    "chloe = Transformer(len(infield.vocab), len(outfield.vocab), \n",
    "                    emb_dim, n_layers, heads, mem_slots, dropout)\n",
    "\n",
    "chloe.controller.reset(batch_size)\n",
    "load_subset_weights(chloe, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_model(input_str, model, opt, infield, outfield):\n",
    "    '''\n",
    "    input:\n",
    "        input_str is a string, it is what you want to say to the dialogue model\n",
    "        model is a encoder, decoder and a last layer linear transformation\n",
    "        opt is an options object with the maximum length of the output sequence opt.max_len\n",
    "        infield and outfield are the data.fields that store the vocabulary\n",
    "    output:\n",
    "        an output string response from the dialogue model\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    in_toks = string2tensor(input_str, infield) # string to tensor \n",
    "    in_mask = (in_toks != infield.vocab.stoi['<pad>']).unsqueeze(-2) #make input mask\n",
    "    \n",
    "    model.incoding = model.memcoder(in_toks, in_mask, model.controller.context, model.controller.mask)\n",
    "    model.controller.memory2context(model.incoding)\n",
    "    \n",
    "    # Initialize decoder ouput as the start token decoder input \n",
    "    init_tok = outfield.vocab.stoi['<sos>'] # this is the integer for the start token\n",
    "    decoder_input = torch.LongTensor([[init_tok]]) # start token to initiate the decoder\n",
    "    \n",
    "    for pos in range(opt.max_len):\n",
    "        # make target mask, pos+1 cause pos starts at 0\n",
    "        decoder_input_mask = nopeak_mask(size=pos+1, opt=opt) \n",
    "        model.dout = model.decoder(decoder_input, decoder_input_mask, model.incoding, in_mask)\n",
    "        out = model.out(model.dout)\n",
    "        softout = F.softmax(out, dim=-1) \n",
    "\n",
    "        distr = Categorical(probs=softout)\n",
    "        action = distr.sample()[:,-1].unsqueeze(0) # sample from that distribution to get next token\n",
    "        decoder_input = torch.cat((decoder_input, action), dim=1) \n",
    "\n",
    "        if outfield.vocab.itos[action] == '<eos>':\n",
    "            de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0][1:-1]])\n",
    "            return de_str\n",
    "        \n",
    "    de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "    return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi bobo !\n",
      "tensor([[[ 0.1757, -0.0697,  0.3409,  0.0694, -0.1293,  0.0456,  0.3600,\n",
      "           0.1633],\n",
      "         [ 0.1502, -0.0755,  0.3560,  0.1028, -0.1073,  0.0203,  0.3796,\n",
      "           0.1731],\n",
      "         [ 0.1426, -0.0790,  0.3639,  0.1227, -0.1037,  0.0104,  0.3839,\n",
      "           0.1745],\n",
      "         [ 0.1360, -0.0829,  0.3770,  0.1473, -0.1023,  0.0115,  0.3865,\n",
      "           0.1750]]])\n",
      "tensor([[0.2476, 0.2475, 0.2507, 0.2542]])\n",
      "you are chloe\n"
     ]
    }
   ],
   "source": [
    "load_subset_weights(chloe, opt)\n",
    "\n",
    "print(talk_to_model(\"my name is bobo\", chloe, opt, infield, outfield))\n",
    "\n",
    "chloe.controller.write2memory(chloe.dout)\n",
    "chloe.controller.memory2context(chloe.dout)\n",
    "print(chloe.controller.memory_module.memory.data)\n",
    "print(chloe.controller.w.data)\n",
    "print(talk_to_model(\"what is my name?\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 0.117\n",
      "0m: epoch 1 loss = 0.073\n",
      "0m: epoch 5 loss = 0.067\n",
      "0m: epoch 6 loss = 0.043\n",
      "0m: epoch 7 loss = 0.038\n",
      "0m: epoch 10 loss = 0.036\n",
      "0m: epoch 20 loss = 0.032\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "load_subset_weights(chloe, opt)\n",
    "\n",
    "conversation_list = [\n",
    "{\"listen\":\"my name is chloe\", \"reply\":\"hi chloe!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"you are chloe\"},\n",
    "{\"listen\":\"my name is fluffy\", \"reply\":\"hey fluffy!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"fluffy pillow\"},\n",
    "{\"listen\":\"my name is snuggles\", \"reply\":\"hello snuggles!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"snuggles the bunny\"},\n",
    "{\"listen\":\"my name is bobo\", \"reply\":\"hi bobo!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"you are bobo\"},\n",
    "                    ]\n",
    "\n",
    "sos_tok = torch.LongTensor([[outfield.vocab.stoi['<sos>']]]) \n",
    "eos_tok = torch.LongTensor([[outfield.vocab.stoi['<eos>']]]) \n",
    "\n",
    "chloe.train()\n",
    "start = time.time()\n",
    "best_loss = 100\n",
    "\n",
    "opt.epochs = 40\n",
    "opt.lr = 0.0005\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.8, patience=4)\n",
    "\n",
    "for epoch in range(opt.epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(conversation_list)):\n",
    "        \n",
    "        # init optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prepare source and targets \n",
    "        listen_string = conversation_list[i][\"listen\"]\n",
    "        reply_string = conversation_list[i][\"reply\"]\n",
    "        listen_toks = string2tensor(listen_string, infield)\n",
    "        reply_toks = string2tensor(reply_string, outfield)\n",
    "        reply_start = torch.cat((sos_tok,reply_toks), dim=1) #teacher forcing \n",
    "        reply_labels = torch.cat((reply_toks,eos_tok), dim=1).contiguous().view(-1) #target\n",
    "        listen_mask, reply_mask = create_masks(listen_toks, reply_start, opt)\n",
    "        \n",
    "        # forward pass and write decoder output to memory\n",
    "        logits = chloe(listen_toks, listen_mask, reply_start, reply_mask)\n",
    "        #chloe.incoding = model.memcoder(listen_toks, listen_mask, chloe.controller.context, chloe.controller.mask)\n",
    "        #chloe.controller.memory2context(chloe.incoding)\n",
    "        #chloe.dout = chloe.decoder(reply_start, reply_mask, self.incoding, in_mask)\n",
    "        #output = self.out(self.dout)\n",
    "        \n",
    "        chloe.controller.write2memory(chloe.dout)\n",
    "        chloe.controller.memory2context(chloe.dout)\n",
    "        \n",
    "        # calculate loss\n",
    "        flat_logits = logits.view(-1, logits.size(-1))\n",
    "        batch_loss = F.cross_entropy(flat_logits, reply_labels, ignore_index = opt.trg_pad)\n",
    "        \n",
    "        # calculate gradients\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        torch.nn.utils.clip_grad_norm_(chloe.parameters(), max_norm = 1.0) \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    epoch_loss = total_loss/len(conversation_list)\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(chloe.state_dict(), opt.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, \n",
    "                                        epoch, epoch_loss))\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >   my name is fluffy   >  hey fluffy !\n",
      " >   what is my name?   >  fluffy pillow\n",
      " >   my name is snuggles  >  hello snuggles !\n",
      " >   what is my name?   >  snuggles the bunny\n",
      " >   my name is bobo   >  hi bobo !\n",
      " >   what is my name?   >  you are bobo\n",
      " >   my name is chloe  >  hi chloe !\n",
      " >   what is my name?   >  you are chloe\n"
     ]
    }
   ],
   "source": [
    "load_subset_weights(chloe, opt)\n",
    "test_list = [\n",
    "    \" my name is fluffy \",\n",
    "    \" what is my name? \",\n",
    "    \" my name is snuggles\",\n",
    "    \" what is my name? \",\n",
    "    \" my name is bobo \",\n",
    "    \" what is my name? \",\n",
    "    \" my name is chloe\",\n",
    "    \" what is my name? \",\n",
    "]\n",
    "\n",
    "for i in test_list:\n",
    "    print(\" > \", i, \" > \",  talk_to_model(i,chloe,opt,infield,outfield))\n",
    "    chloe.controller.write2memory(chloe.dout)\n",
    "    chloe.controller.memory2context(chloe.dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
