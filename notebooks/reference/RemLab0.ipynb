{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import *\n",
    "from EncoderDecoder import *\n",
    "from Talk import *\n",
    "from Trainer import *\n",
    "from LearningDynamics import *\n",
    "\n",
    "#from Beam import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTransformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, \n",
    "                 heads, mem_slots, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        dim_k = emb_dim // heads\n",
    "        self.mem_slots = mem_slots\n",
    "        self.batch_size = None\n",
    "        self.memory = None\n",
    "        \n",
    "        self.encoder = Encoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "\n",
    "        self.mhdpa = MultiHeadAttention(heads, emb_dim, dim_k, dropout)\n",
    "        self.z_gate = nn.Linear(emb_dim, emb_dim)\n",
    "        self.norm_mem = Norm(emb_dim)\n",
    "        \n",
    "    def update_memory(self):\n",
    "        #print(self.memory.shape, self.d_output.shape)\n",
    "        if self.memory.size(0) < self.d_output.size(0):\n",
    "            self.memory = self.memory.repeat(self.d_output.size(0), 1, 1)\n",
    "        mem_dialogue = torch.cat([self.memory, self.d_output], dim=-2) \n",
    "        new_memory, _ = self.mhdpa(self.memory, mem_dialogue, mem_dialogue)\n",
    "        new_mem_norm = self.norm_mem(new_memory + self.memory)\n",
    "        z_t = torch.sigmoid(self.z_gate(self.memory))\n",
    "        self.memory = (1 - z_t)*self.memory + z_t*new_mem_norm\n",
    "         \n",
    "    def concat_mem(self, in_encoded, mask):\n",
    "        if isinstance(self.memory, torch.Tensor):\n",
    "            #print(in_encoded.shape, self.memory.shape)\n",
    "            if in_encoded.size(0) < self.memory.size(0):\n",
    "                in_encoded = in_encoded.repeat(self.memory.size(0), 1, 1)\n",
    "            in_mem_encoded = torch.cat([in_encoded, self.memory], dim=-2) \n",
    "            mask=torch.from_numpy(np.ones((1,in_mem_encoded.size(-2))).astype('uint8'))==1\n",
    "            mask=torch.stack([mask for _ in range(in_mem_encoded.size(0))])\n",
    "            return in_mem_encoded, mask\n",
    "        else:\n",
    "            self.memory = in_encoded\n",
    "            return in_encoded, mask \n",
    "        \n",
    "    def repackage_hidden(self, h):\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        elif h == None:\n",
    "            return None\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        self.memory = self.repackage_hidden(self.memory)\n",
    "        in_encoded = self.encoder(in_toks, in_mask)\n",
    "        in_encoded, in_mask = self.concat_mem(in_encoded, in_mask)\n",
    "        self.d_output = self.decoder(out_toks, out_mask, in_encoded, in_mask)\n",
    "        output = self.out(self.d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_model(input_str, model, opt, infield, outfield):\n",
    "    '''\n",
    "    input:\n",
    "        input_str is a string, it is what you want to say to the dialogue model\n",
    "        model is a encoder, decoder and a last layer linear transformation\n",
    "        opt is an options object with the maximum length of the output sequence opt.max_len\n",
    "        infield and outfield are the data.fields that store the vocabulary\n",
    "    output:\n",
    "        an output string response from the dialogue model\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    input_sequence = string2tensor(input_str, infield) # string to tensor \n",
    "    input_mask = (input_sequence != infield.vocab.stoi['<pad>']).unsqueeze(-2) #make input mask\n",
    "    encoding = model.encoder(input_sequence, input_mask)\n",
    "    init_tok = outfield.vocab.stoi['<sos>'] # this is the integer for the start token\n",
    "    decoder_input = torch.LongTensor([[init_tok]]) # use start token to initiate the decoder\n",
    "    \n",
    "    for pos in range(opt.max_len):\n",
    "        decoder_input_mask = nopeak_mask(size=pos+1, opt=opt) # make target mask, pos+1 casue pos starts at 0\n",
    "        out = model.out(model.decoder(decoder_input, decoder_input_mask, encoding, input_mask))\n",
    "        softout = F.softmax(out, dim=-1) \n",
    "\n",
    "        distr = Categorical(probs=softout)\n",
    "        action = distr.sample()[:,-1].unsqueeze(0) # sample from that distribution to get next token\n",
    "        decoder_input = torch.cat((decoder_input, action), dim=1) \n",
    "\n",
    "        if outfield.vocab.itos[action] == '<eos>':\n",
    "            de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0][1:-1]])\n",
    "            return de_str\n",
    "        \n",
    "    de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "    return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey fluffy !\n"
     ]
    }
   ],
   "source": [
    "opt = Options(batchsize=1, device = torch.device(\"cpu\"), epochs=20, lr=0.0001, \n",
    "              max_len = 25, save_path = '../saved/weights/memory_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../saved/memory.json', opt=opt)\n",
    "\n",
    "emb_dim, n_layers, heads, mem_slots, dropout = 32, 2, 2, 3, 0.01 \n",
    "chloe = MemoryTransformer(len(infield.vocab), len(outfield.vocab), \n",
    "                          emb_dim, n_layers, heads, mem_slots, dropout)\n",
    "\n",
    "load_subset_weights(chloe, opt)\n",
    "print(talk_to_model(\"my name is fluffy\", chloe, opt, infield, outfield))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >   my name is fluffy   >  hey fluffy!\n",
      " >   what is my name?   >  hey fluffy!\n",
      " >   my name is fluffy what is my name?  >  hey fluffy!\n",
      " >   my name is snuggles  >  hello snuggles!\n",
      " >   what is my name?   >  hey fluffy!\n",
      " >   my name is snuggles what is my name?   >  hello snuggles!\n",
      " >   my name is bobo   >  hi bobo!\n",
      " >   what is my name?   >  hey fluffy!\n",
      " >   my name is bobo what is my name?   >  <unk> <unk> bobo\n"
     ]
    }
   ],
   "source": [
    "#scheduler = CosineWithRestarts(optimizer, T_max=len(conversation_list))\n",
    "load_subset_weights(chloe, opt)\n",
    "chloe.eval()\n",
    "\n",
    "test_list = [\n",
    "    \" my name is fluffy \",\n",
    "    \" what is my name? \",\n",
    "    \" my name is fluffy what is my name?\",\n",
    "    \" my name is snuggles\",\n",
    "    \" what is my name? \",\n",
    "    \" my name is snuggles what is my name? \",\n",
    "    \" my name is bobo \",\n",
    "    \" what is my name? \",\n",
    "    \" my name is bobo what is my name? \"\n",
    "]\n",
    "\n",
    "opt.k = 10\n",
    "\n",
    "for i in test_list:\n",
    "    print(\" > \", i, \" > \",  translate_sentence(i,chloe,opt,infield,outfield))\n",
    "    chloe.update_memory() # Update Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 0.765\n",
      "0m: epoch 1 loss = 0.399\n",
      "0m: epoch 2 loss = 0.152\n",
      "0m: epoch 3 loss = 0.132\n",
      "0m: epoch 4 loss = 0.129\n",
      "0m: epoch 8 loss = 0.128\n",
      "0m: epoch 17 loss = 0.113\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation_list = [\n",
    "{\"listen\":\"my name is fluffy\", \"reply\":\"hey fluffy!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"fluffy pillow\"},\n",
    "{\"listen\":\"my name is fluffy what is my name?\", \"reply\":\"fluffy pillow\"},\n",
    "{\"listen\":\"my name is snuggles\", \"reply\":\"hello snuggles!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"snuggles the bunny\"},\n",
    "{\"listen\":\"my name is snuggles what is my name?\", \"reply\":\"snuggles the bunny\"},\n",
    "{\"listen\":\"my name is bobo\", \"reply\":\"hi bobo!\"},\n",
    "{\"listen\":\"what is my name?\", \"reply\":\"you are bobo\"},\n",
    "{\"listen\":\"my name is bobo what is my name?\", \"reply\":\"you are bobo\"},\n",
    "                    ]\n",
    "\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.6, patience=3)\n",
    "\n",
    "sos_tok = torch.LongTensor([[outfield.vocab.stoi['<sos>']]]) \n",
    "eos_tok = torch.LongTensor([[outfield.vocab.stoi['<eos>']]]) \n",
    "\n",
    "chloe.train()\n",
    "start = time.time()\n",
    "best_loss = 100\n",
    "opt.epochs = 50 \n",
    "for epoch in range(opt.epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(conversation_list)):\n",
    "        listen_string = conversation_list[i][\"listen\"]\n",
    "        reply_string = conversation_list[i][\"reply\"]\n",
    "        listen_toks = string2tensor(listen_string, infield)\n",
    "        reply_toks = string2tensor(reply_string, outfield)\n",
    "        reply_start = torch.cat((sos_tok,reply_toks), dim=1)\n",
    "        reply_labels = torch.cat((reply_toks,eos_tok), dim=1).contiguous().view(-1)\n",
    "        \n",
    "        listen_mask, reply_mask = create_masks(listen_toks, reply_start, opt)\n",
    "        \n",
    "        logits = chloe(listen_toks, listen_mask, reply_start, reply_mask)\n",
    "        \n",
    "        chloe.update_memory() # Update Memory\n",
    "        \n",
    "        flat_logits = logits.view(-1, logits.size(-1))\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = F.cross_entropy(flat_logits, reply_labels, ignore_index = opt.trg_pad)\n",
    "\n",
    "        batch_loss.backward() #batch_loss.backward(retain_graph=True) #\n",
    "        torch.nn.utils.clip_grad_norm_(chloe.parameters(), max_norm = 1.0) \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    epoch_loss = total_loss/len(conversation_list)\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(chloe.state_dict(), opt.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, \n",
    "                                        epoch, epoch_loss))\n",
    "        \n",
    "    if epoch_loss < 0.09:\n",
    "        break\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to train the memory. How do we do this? we need to talk to the model and allow it to accumulate at least one cycle of conversation, then teach it to respond correctly given the previous listen-reply exchange"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
