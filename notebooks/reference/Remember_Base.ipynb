{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, os, datetime, shutil, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from MoveData import *\n",
    "from EncoderDecoder import *\n",
    "from Elements import * \n",
    "from Talk import *\n",
    "from Trainer import *\n",
    "from LearningDynamics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTransformer(nn.Module):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, emb_dim, n_layers, \n",
    "                 heads, mem_slots, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = None\n",
    "        dim_k = emb_dim // heads\n",
    "        self.mem_slots = mem_slots\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.encoder = Decoder(in_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.decoder = Decoder(out_vocab_size, emb_dim, n_layers, heads, dropout)\n",
    "        self.out = nn.Linear(emb_dim, out_vocab_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.memory = torch.randn((1,self.mem_slots, emb_dim))\n",
    "        '''  \n",
    "        self.memory = torch.stack([torch.eye(self.mem_slots) for _ in range(1)])\n",
    "        if emb_dim > self.mem_slots:\n",
    "            difference = emb_dim - self.mem_slots\n",
    "            pad = torch.zeros((1, self.mem_slots, difference))\n",
    "            self.memory = torch.cat([self.memory, pad], -1)\n",
    "        elif emb_dim < self.mem_slots:\n",
    "            self.memory = self.memory[:, :, :emb_dim]\n",
    "        '''\n",
    "        self.mem_mask = torch.ones(1,1,self.mem_slots) == 1\n",
    "        \n",
    "        self.mem_update = MultiHeadAttention(heads, emb_dim, dim_k, dropout)\n",
    "        self.normalizeMemory1 = Norm(emb_dim)\n",
    "        self.z_gate = nn.Linear(emb_dim*2, emb_dim)\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        \n",
    "    def update_memory(self):\n",
    "        mem_dialogue = torch.cat([self.memory, self.e_output, self.d_output], dim=-2) \n",
    "        #new_memory, scores = self.mem_update(self.memory, mem_dialogue, mem_dialogue)\n",
    "        self.memory, scores = self.mem_update(self.memory, mem_dialogue, mem_dialogue)\n",
    "        #new_mem_norm = self.normalizeMemory1(new_memory + self.memory)\n",
    "        #z_t = torch.sigmoid(self.z_gate(torch.cat([self.memory, new_mem_norm], dim=-1))) \n",
    "        #self.memory = (1 - z_t)*self.memory + z_t*new_mem_norm\n",
    "        \n",
    "    def forward(self, in_toks, in_mask, out_toks, out_mask):\n",
    "        self.memory = self.repackage_hidden(self.memory)  \n",
    "        self.mem_mask = self.repackage_hidden(self.mem_mask)  \n",
    "        self.e_output = self.encoder(in_toks, in_mask, self.memory, self.mem_mask)\n",
    "        self.d_output = self.decoder(out_toks, out_mask, self.e_output, in_mask)\n",
    "        output = self.out(self.d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_model(input_str, model, opt, infield, outfield):\n",
    "    '''\n",
    "    input:\n",
    "        input_str is a string, it is what you want to say to the dialogue model\n",
    "        model is a encoder, decoder and a last layer linear transformation\n",
    "        opt is an options object with the maximum length of the output sequence opt.max_len\n",
    "        infield and outfield are the data.fields that store the vocabulary\n",
    "    output:\n",
    "        an output string response from the dialogue model\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    input_sequence = string2tensor(input_str, infield) # string to tensor \n",
    "    input_mask = (input_sequence != infield.vocab.stoi['<pad>']).unsqueeze(-2) #make input mask\n",
    "    model.e_output = model.encoder(input_sequence, input_mask, model.memory, model.mem_mask)\n",
    "    init_tok = outfield.vocab.stoi['<sos>'] # this is the integer for the start token\n",
    "    decoder_input = torch.LongTensor([[init_tok]]) # use start token to initiate the decoder\n",
    "    \n",
    "    for pos in range(opt.max_len):\n",
    "        decoder_input_mask = nopeak_mask(size=pos+1, opt=opt) # make target mask, pos+1 casue pos starts at 0\n",
    "        model.d_output = model.decoder(decoder_input, decoder_input_mask, model.e_output, input_mask)\n",
    "        out = model.out(model.d_output)\n",
    "        softout = F.softmax(out, dim=-1) \n",
    "\n",
    "        distr = Categorical(probs=softout)\n",
    "        action = distr.sample()[:,-1].unsqueeze(0) # sample from that distribution to get next token\n",
    "        decoder_input = torch.cat((decoder_input, action), dim=1) \n",
    "\n",
    "        if outfield.vocab.itos[action] == '<eos>':\n",
    "            de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0][1:-1]])\n",
    "            return de_str\n",
    "        \n",
    "    de_str = ' '.join([outfield.vocab.itos[tok] for tok in decoder_input[0]])\n",
    "    return de_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4841, -0.0458,  1.2232,  0.0397, -2.1136, -0.5114,  0.5870, -0.9800],\n",
      "        [-0.8795, -0.2978,  1.3578, -1.0898, -0.5419, -1.7764,  0.3256,  0.7855],\n",
      "        [-0.4614,  0.4574,  1.4177, -0.8479,  0.9467,  0.0997,  0.4975, -0.1688],\n",
      "        [ 1.2256,  0.1544, -0.6595,  0.8973, -0.0573, -1.2790, -0.1369,  0.1134]])\n",
      "\n",
      "tensor([[-0.0134,  0.0326,  0.1355, -0.1515,  0.4848,  0.0264,  0.2180, -0.2135],\n",
      "        [-0.0732, -0.0125,  0.1349, -0.0699,  0.4909, -0.0727,  0.2234, -0.1524],\n",
      "        [-0.0239, -0.0292,  0.1792, -0.0644,  0.4947, -0.0371,  0.2724, -0.1650],\n",
      "        [ 0.0107,  0.0119,  0.1586, -0.1304,  0.4849,  0.0373,  0.2529, -0.2087]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "hi bobo taco <pad> <unk> bobo <sos> hi <pad> taco hi hello hello hi hi\n",
      "tensor([[ 0.0932,  0.1204,  0.0775, -0.1688,  0.4691, -0.0655, -0.0106, -0.0411],\n",
      "        [ 0.0927,  0.1201,  0.0777, -0.1688,  0.4695, -0.0678, -0.0110, -0.0408],\n",
      "        [ 0.0921,  0.1200,  0.0771, -0.1685,  0.4696, -0.0668, -0.0111, -0.0412],\n",
      "        [ 0.0927,  0.1203,  0.0770, -0.1688,  0.4693, -0.0652, -0.0108, -0.0416]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "opt = Options(batchsize=1, device = torch.device(\"cpu\"), epochs=20, lr=0.01, \n",
    "              max_len = 25, save_path = '../saved/weights/memory_weights')\n",
    "\n",
    "data_iter, infield, outfield, opt = json2datatools(path='../saved/memory.json', opt=opt)\n",
    "\n",
    "emb_dim, n_layers, heads, mem_slots, dropout = 8, 1, 4, 4, 0.0\n",
    "\n",
    "chloe = MemoryTransformer(len(infield.vocab), len(outfield.vocab), emb_dim, n_layers, heads, mem_slots, dropout)\n",
    "\n",
    "\n",
    "print(chloe.memory[0])\n",
    "print(talk_to_model(\"my name is bobo\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(chloe.memory[0])\n",
    "print(talk_to_model(\"what is my name\", chloe, opt, infield, outfield))\n",
    "chloe.update_memory()\n",
    "print(chloe.memory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m: epoch 0 loss = 2.273\n",
      "tensor([[-0.0274,  0.0043,  0.0720, -0.0387,  0.4485, -0.1252,  0.1340, -0.0538],\n",
      "        [-0.0274,  0.0043,  0.0720, -0.0387,  0.4485, -0.1252,  0.1340, -0.0538],\n",
      "        [-0.0274,  0.0043,  0.0720, -0.0387,  0.4485, -0.1252,  0.1340, -0.0538],\n",
      "        [-0.0274,  0.0043,  0.0720, -0.0387,  0.4485, -0.1252,  0.1340, -0.0538]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "0m: epoch 1 loss = 1.607\n",
      "tensor([[-0.0637, -0.0379,  0.0776,  0.0279,  0.4715, -0.1717,  0.1498, -0.0385],\n",
      "        [-0.0637, -0.0379,  0.0776,  0.0279,  0.4715, -0.1717,  0.1498, -0.0385],\n",
      "        [-0.0637, -0.0379,  0.0776,  0.0279,  0.4715, -0.1717,  0.1498, -0.0385],\n",
      "        [-0.0637, -0.0379,  0.0776,  0.0279,  0.4715, -0.1717,  0.1498, -0.0385]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "0m: epoch 2 loss = 1.315\n",
      "tensor([[-0.0945, -0.0718,  0.0657,  0.0864,  0.4979, -0.1910,  0.1399, -0.0256],\n",
      "        [-0.0945, -0.0718,  0.0657,  0.0864,  0.4979, -0.1910,  0.1399, -0.0256],\n",
      "        [-0.0945, -0.0718,  0.0657,  0.0864,  0.4979, -0.1910,  0.1399, -0.0256],\n",
      "        [-0.0945, -0.0718,  0.0657,  0.0864,  0.4979, -0.1910,  0.1399, -0.0256]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "0m: epoch 3 loss = 1.131\n",
      "tensor([[-0.1132, -0.0799,  0.0462,  0.1068,  0.5023, -0.2005,  0.1254, -0.0129],\n",
      "        [-0.1132, -0.0799,  0.0462,  0.1068,  0.5023, -0.2005,  0.1254, -0.0129],\n",
      "        [-0.1132, -0.0799,  0.0462,  0.1068,  0.5023, -0.2005,  0.1254, -0.0129],\n",
      "        [-0.1132, -0.0799,  0.0462,  0.1068,  0.5023, -0.2005,  0.1254, -0.0129]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "0m: epoch 4 loss = 1.035\n",
      "tensor([[-0.1251, -0.0938,  0.0314,  0.1464,  0.5263, -0.2207,  0.0851,  0.0137],\n",
      "        [-0.1251, -0.0938,  0.0314,  0.1464,  0.5263, -0.2207,  0.0851,  0.0137],\n",
      "        [-0.1251, -0.0938,  0.0314,  0.1464,  0.5263, -0.2207,  0.0851,  0.0137],\n",
      "        [-0.1251, -0.0938,  0.0314,  0.1464,  0.5263, -0.2207,  0.0851,  0.0137]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "0m: epoch 5 loss = 0.965\n",
      "tensor([[-0.1473, -0.1323,  0.0240,  0.2338,  0.5878, -0.2654,  0.0214,  0.0540],\n",
      "        [-0.1473, -0.1323,  0.0240,  0.2338,  0.5878, -0.2654,  0.0214,  0.0540],\n",
      "        [-0.1473, -0.1323,  0.0240,  0.2338,  0.5878, -0.2654,  0.0214,  0.0540],\n",
      "        [-0.1473, -0.1323,  0.0240,  0.2338,  0.5878, -0.2654,  0.0214,  0.0540]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "0m: epoch 6 loss = 0.880\n",
      "tensor([[-0.1797, -0.1700,  0.0090,  0.3225,  0.6603, -0.3232, -0.0596,  0.0939],\n",
      "        [-0.1797, -0.1700,  0.0090,  0.3225,  0.6603, -0.3232, -0.0596,  0.0939],\n",
      "        [-0.1797, -0.1700,  0.0090,  0.3225,  0.6603, -0.3232, -0.0596,  0.0939],\n",
      "        [-0.1797, -0.1700,  0.0090,  0.3225,  0.6603, -0.3232, -0.0596,  0.0939]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "0m: epoch 7 loss = 0.817\n",
      "tensor([[-0.1689, -0.1490, -0.0024,  0.2937,  0.6434, -0.3280, -0.0742,  0.1009],\n",
      "        [-0.1689, -0.1490, -0.0024,  0.2937,  0.6434, -0.3280, -0.0742,  0.1009],\n",
      "        [-0.1689, -0.1490, -0.0024,  0.2937,  0.6434, -0.3280, -0.0742,  0.1009],\n",
      "        [-0.1689, -0.1490, -0.0024,  0.2937,  0.6434, -0.3280, -0.0742,  0.1009]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "0m: epoch 8 loss = 0.772\n",
      "tensor([[-0.1595, -0.1346, -0.0062,  0.2785,  0.6367, -0.3259, -0.0887,  0.1076],\n",
      "        [-0.1595, -0.1346, -0.0062,  0.2785,  0.6367, -0.3259, -0.0887,  0.1076],\n",
      "        [-0.1595, -0.1346, -0.0062,  0.2785,  0.6367, -0.3259, -0.0887,  0.1076],\n",
      "        [-0.1595, -0.1346, -0.0062,  0.2785,  0.6367, -0.3259, -0.0887,  0.1076]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "0m: epoch 9 loss = 0.733\n",
      "tensor([[-0.1540, -0.1379, -0.0038,  0.2759,  0.6336, -0.3138, -0.0794,  0.1027],\n",
      "        [-0.1540, -0.1379, -0.0038,  0.2759,  0.6336, -0.3138, -0.0794,  0.1027],\n",
      "        [-0.1540, -0.1379, -0.0038,  0.2759,  0.6336, -0.3138, -0.0794,  0.1027],\n",
      "        [-0.1540, -0.1379, -0.0038,  0.2759,  0.6336, -0.3138, -0.0794,  0.1027]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation_list = [\n",
    "    {\"listen\":\"hello\", \"reply\":\"hi\"},\n",
    "    {\"listen\":\"im sunggles\", \"reply\":\"hello snuggles\"},\n",
    "    {\"listen\":\"whats my name\", \"reply\":\"snuggles\"},\n",
    "    {\"listen\":\"im bobo\", \"reply\":\"bobo\"},\n",
    "    {\"listen\":\"whats my name\", \"reply\":\"bobo\"},\n",
    "    {\"listen\":\"im taco\", \"reply\":\"hello taco\"},\n",
    "    {\"listen\":\"whats my name\", \"reply\":\"taco\"},\n",
    "    {\"listen\":\"my name is fluffy\", \"reply\":\"hello fluffy\"},\n",
    "    {\"listen\":\"whats my name\", \"reply\":\"fluffy\"},\n",
    "                    ]\n",
    "\n",
    "opt.lr = 0.01\n",
    "opt.epochs = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(chloe.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.7, patience=5)\n",
    "\n",
    "sos_tok = torch.LongTensor([[outfield.vocab.stoi['<sos>']]]) \n",
    "eos_tok = torch.LongTensor([[outfield.vocab.stoi['<eos>']]]) \n",
    "\n",
    "chloe.train()\n",
    "start = time.time()\n",
    "best_loss = 100\n",
    "\n",
    "for epoch in range(opt.epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(conversation_list)):\n",
    "        listen_string = conversation_list[i][\"listen\"]\n",
    "        reply_string = conversation_list[i][\"reply\"]\n",
    "        listen_toks = string2tensor(listen_string, infield)\n",
    "        reply_toks = string2tensor(reply_string, outfield)\n",
    "        reply_start = torch.cat((sos_tok,reply_toks), dim=1)\n",
    "        reply_labels = torch.cat((reply_toks, eos_tok), dim=1).contiguous().view(-1)\n",
    "        \n",
    "        listen_mask, reply_mask = create_masks(listen_toks, reply_start, opt)\n",
    "        \n",
    "        logits = chloe(listen_toks, listen_mask, reply_start, reply_mask)\n",
    "        \n",
    "        chloe.update_memory()\n",
    "        \n",
    "        flat_logits = logits.view(-1, logits.size(-1))\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss=F.cross_entropy(flat_logits,reply_labels,ignore_index=opt.trg_pad)\n",
    "        batch_loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(chloe.parameters(), max_norm = 1.0) \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    epoch_loss = total_loss/len(conversation_list)\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(chloe.state_dict(), opt.save_path)\n",
    "        print(\"%dm: epoch %d loss = %.3f\" %((time.time() - start)//60, epoch, epoch_loss))\n",
    "        print(chloe.memory[0])\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "print(\"finished\")\n",
    "\n",
    "load_subset_weights(chloe, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1513,  0.4629,  0.5391, -0.7875,  0.3189,  1.0446, -0.5672, -0.4214],\n",
      "        [-1.2432,  1.0948,  0.5867, -2.5005, -0.4963,  0.3767, -0.7254, -0.2532],\n",
      "        [ 1.6809, -0.3526, -0.0733, -1.1703,  0.1123, -0.2663, -0.3607,  0.8543],\n",
      "        [ 0.2006,  0.8663, -1.2880, -1.7130, -0.4821, -0.1936,  2.7281,  0.2105],\n",
      "        [ 1.0299, -0.1425,  0.8465,  1.0078, -0.7210, -1.8339,  1.4362, -0.3073]])\n",
      "----------------------------------------------------------------------\n",
      "tensor([[-0.1576,  0.2983, -0.0238, -0.6226, -0.2082,  0.6605,  0.4974, -0.2098],\n",
      "        [-0.1808,  0.1269, -0.0847, -0.8431, -0.1231,  0.8690,  0.6174, -0.3190],\n",
      "        [-0.2514,  0.1243, -0.0298, -0.7493, -0.1235,  0.8246,  0.6049, -0.2453],\n",
      "        [-0.2788, -0.0535, -0.0585, -0.8879, -0.0593,  0.7805,  0.5836, -0.1028]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "tensor([[-1.6891,  0.7308,  0.5370, -0.9806,  0.2180,  1.4749,  0.0758, -0.3667],\n",
      "        [-0.6852,  1.0633,  0.5877, -1.9538, -0.1534,  1.0792,  0.1845, -0.1222],\n",
      "        [ 1.4092, -0.3122, -0.1822, -2.0685, -0.0867,  0.5047,  0.1785,  0.5573],\n",
      "        [-0.0636,  0.4527, -0.7984, -1.5252, -0.3319,  0.3218,  1.9004,  0.0442]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "tensor([[-1.8623,  0.6155,  0.5386, -0.8842,  0.2670,  1.2091, -0.1474, -0.3994],\n",
      "        [-1.0032,  1.0792,  0.5869, -2.2451, -0.3411,  0.6814, -0.2236, -0.1959],\n",
      "        [ 1.5853, -0.3341, -0.1276, -1.6151,  0.0455, -0.0033, -0.1888,  0.7242],\n",
      "        [ 0.0487,  0.5886, -1.0479, -1.6460, -0.4475,  0.1415,  2.2750,  0.1186]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "in_vec = torch.randn(1,1,chloe.emb_dim)\n",
    "mem_dialogue = torch.cat([chloe.memory, in_vec], dim=-2) \n",
    "print(mem_dialogue[0])\n",
    "new_memory, scores = chloe.mem_update(chloe.memory, mem_dialogue, mem_dialogue)\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(new_memory[0])\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "new_mem_norm = chloe.normalizeMemory1(new_memory + chloe.memory)\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(new_mem_norm[0])\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "z_t = torch.sigmoid(chloe.z_gate(torch.cat([chloe.memory, new_mem_norm], dim=-1))) \n",
    "chloe.memory = (1 - z_t)*chloe.memory + z_t*new_mem_norm\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(chloe.memory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1629, -2.0674,  1.2915, -0.1818,  0.0793, -0.3673,  0.1813,  0.8741],\n",
       "        [ 0.1629, -2.0674,  1.2915, -0.1818,  0.0793, -0.3673,  0.1813,  0.8741],\n",
       "        [ 0.1629, -2.0674,  1.2915, -0.1818,  0.0793, -0.3673,  0.1813,  0.8741],\n",
       "        [ 0.1629, -2.0674,  1.2915, -0.1818,  0.0793, -0.3673,  0.1813,  0.8741]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >  hello  >  hi\n",
      " >  im sunggles  >  hello snuggles\n",
      " >  whats my name  >  snuggles\n",
      " >  im bobo  >  bobo\n",
      " >  whats my name  >  fluffy\n",
      " >  im taco  >  hello taco\n",
      " >  whats my name  >  snuggles\n",
      " >  my name is fluffy  >  hello fluffy\n",
      " >  whats my name  >  bobo\n"
     ]
    }
   ],
   "source": [
    "load_subset_weights(chloe, opt) \n",
    "#scheduler = CosineWithRestarts(optimizer, T_max=len(conversation_list))\n",
    "\n",
    "chloe.eval()\n",
    "\n",
    "test_list = [\n",
    "    \"hello\", \n",
    "    \"im sunggles\",\n",
    "    \"whats my name\", \n",
    "    \"im bobo\", \n",
    "    \"whats my name\", \n",
    "    \"im taco\", \n",
    "    \"whats my name\", \n",
    "    \"my name is fluffy\",\n",
    "    \"whats my name\", \n",
    "]\n",
    "\n",
    "for i in test_list:\n",
    "    print(\" > \", i, \" > \",  talk_to_model(i,chloe,opt,infield,outfield))\n",
    "    chloe.update_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You > my name is fluffy\n",
      "Chloe > hey fluffy !\n",
      "\n",
      "You > what is my name\n",
      "Chloe > you are bobo\n",
      "\n",
      "You > my name is fluffy\n",
      "Chloe > hey fluffy !\n",
      "\n",
      "You > what is my name\n",
      "Chloe > you are bobo\n",
      "\n",
      "You > what is my name?\n",
      "Chloe > taco\n",
      "\n",
      "You > whats my name\n",
      "Chloe > so , how are you ?\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-584-2338a2337335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0mtell_chloe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You > \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0mchloes_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtalk_to_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtell_chloe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchloe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mchloe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"bye chloe\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtell_chloe\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"bye ttyl\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchloes_reply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/chloe/chloebot/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " while True:\n",
    "    tell_chloe = input(\"You > \")\n",
    "    chloes_reply = talk_to_model(tell_chloe, chloe, opt, infield, outfield)\n",
    "    chloe.update_memory()\n",
    "    if (\"bye chloe\" in tell_chloe or \"bye ttyl\" in chloes_reply):\n",
    "        print('Chloe > '+ chloes_reply + '\\n')\n",
    "        break\n",
    "    else:\n",
    "        print('Chloe > '+ chloes_reply + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to train the memory. How do we do this? we need to talk to the model and allow it to accumulate at least one cycle of conversation, then teach it to respond correctly given the previous listen-reply exchange"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
